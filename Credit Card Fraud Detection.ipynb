{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# European cardholders 2013(September) Transaction Data- Identify fraudulent credit card transactions\n",
    "#### By: Swati Kohli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook evaluates input variables which are the result of a PCA transformation of transactions(privacy concerns) that occurred in two days. Various predictive models are used to see how accurate they are in detecting whether a transaction is a normal payment or a fraud. \n",
    "The goal is to compare 5 models, and see which gives the best performance. To be fair, evaluation is performed using the same holdout set. The five models are:\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Bagged Trees\n",
    "4. Random Forest\n",
    "5. Boosted Trees\n",
    "\n",
    "The dataset is highly unbalanced due to which accuracy is not a good metric to use here. Therefore, F1-score is used for hyperparameter selection for all models. ROC curve and AUC score is also used because it is a good method to use when comparing multiple classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>149938</td>\n",
       "      <td>-0.445664</td>\n",
       "      <td>0.565437</td>\n",
       "      <td>0.733374</td>\n",
       "      <td>-0.999763</td>\n",
       "      <td>1.609578</td>\n",
       "      <td>1.187041</td>\n",
       "      <td>1.452018</td>\n",
       "      <td>-0.198439</td>\n",
       "      <td>-0.048416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195318</td>\n",
       "      <td>-0.107752</td>\n",
       "      <td>-0.100277</td>\n",
       "      <td>-0.329253</td>\n",
       "      <td>-0.375723</td>\n",
       "      <td>0.275200</td>\n",
       "      <td>-0.349132</td>\n",
       "      <td>-0.338845</td>\n",
       "      <td>50.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>148077</td>\n",
       "      <td>0.111788</td>\n",
       "      <td>1.124121</td>\n",
       "      <td>-0.191350</td>\n",
       "      <td>-0.443245</td>\n",
       "      <td>0.777409</td>\n",
       "      <td>-1.071823</td>\n",
       "      <td>1.068130</td>\n",
       "      <td>-0.203815</td>\n",
       "      <td>-0.246504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297632</td>\n",
       "      <td>-0.660398</td>\n",
       "      <td>0.144101</td>\n",
       "      <td>1.072859</td>\n",
       "      <td>-0.444134</td>\n",
       "      <td>0.079985</td>\n",
       "      <td>0.229395</td>\n",
       "      <td>0.091531</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>121831</td>\n",
       "      <td>2.087566</td>\n",
       "      <td>-1.134330</td>\n",
       "      <td>-0.593377</td>\n",
       "      <td>-1.058366</td>\n",
       "      <td>-1.104703</td>\n",
       "      <td>-0.413032</td>\n",
       "      <td>-1.112383</td>\n",
       "      <td>0.110687</td>\n",
       "      <td>-0.074055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053867</td>\n",
       "      <td>-0.082683</td>\n",
       "      <td>0.328018</td>\n",
       "      <td>-0.543838</td>\n",
       "      <td>-0.593097</td>\n",
       "      <td>-0.466010</td>\n",
       "      <td>-0.010324</td>\n",
       "      <td>-0.052221</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>75683</td>\n",
       "      <td>-0.488630</td>\n",
       "      <td>1.039124</td>\n",
       "      <td>1.413263</td>\n",
       "      <td>0.110440</td>\n",
       "      <td>0.205570</td>\n",
       "      <td>-0.721798</td>\n",
       "      <td>0.677329</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>-0.568263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189721</td>\n",
       "      <td>-0.439538</td>\n",
       "      <td>0.084214</td>\n",
       "      <td>0.374691</td>\n",
       "      <td>-0.235030</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.262267</td>\n",
       "      <td>0.102935</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>46080</td>\n",
       "      <td>-1.018049</td>\n",
       "      <td>0.727356</td>\n",
       "      <td>1.723606</td>\n",
       "      <td>-1.409580</td>\n",
       "      <td>0.255078</td>\n",
       "      <td>-0.648925</td>\n",
       "      <td>0.807408</td>\n",
       "      <td>0.190881</td>\n",
       "      <td>-0.605563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425400</td>\n",
       "      <td>-1.383973</td>\n",
       "      <td>0.085528</td>\n",
       "      <td>0.175845</td>\n",
       "      <td>-0.104718</td>\n",
       "      <td>0.462432</td>\n",
       "      <td>0.159748</td>\n",
       "      <td>0.099125</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  149938 -0.445664  0.565437  0.733374 -0.999763  1.609578  1.187041   \n",
       "1  148077  0.111788  1.124121 -0.191350 -0.443245  0.777409 -1.071823   \n",
       "2  121831  2.087566 -1.134330 -0.593377 -1.058366 -1.104703 -0.413032   \n",
       "3   75683 -0.488630  1.039124  1.413263  0.110440  0.205570 -0.721798   \n",
       "4   46080 -1.018049  0.727356  1.723606 -1.409580  0.255078 -0.648925   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  1.452018 -0.198439 -0.048416  ... -0.195318 -0.107752 -0.100277 -0.329253   \n",
       "1  1.068130 -0.203815 -0.246504  ... -0.297632 -0.660398  0.144101  1.072859   \n",
       "2 -1.112383  0.110687 -0.074055  ...  0.053867 -0.082683  0.328018 -0.543838   \n",
       "3  0.677329 -0.007125 -0.568263  ... -0.189721 -0.439538  0.084214  0.374691   \n",
       "4  0.807408  0.190881 -0.605563  ... -0.425400 -1.383973  0.085528  0.175845   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0 -0.375723  0.275200 -0.349132 -0.338845   50.19      0  \n",
       "1 -0.444134  0.079985  0.229395  0.091531    5.37      0  \n",
       "2 -0.593097 -0.466010 -0.010324 -0.052221   40.00      0  \n",
       "3 -0.235030  0.077754  0.262267  0.102935   12.99      0  \n",
       "4 -0.104718  0.462432  0.159748  0.099125   19.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data set\n",
    "fraud = pd.read_csv('fraud.csv')\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 31 columns. The first 30 columns are predictors and the last column (Class) is response. There are no variable names to these columns, since they are PCA transformation of an even earlier data set. For this analysis, Time column is removed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2500\n",
       "1     492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the Time column\n",
    "fraud.drop('Time', axis = 1, inplace = True)\n",
    "\n",
    "# Look at distribution of Class variable\n",
    "fraud.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of the Class variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and response\n",
    "X = fraud.drop('Class', axis = 1)\n",
    "y = fraud.Class\n",
    "\n",
    "# Split data into training and holdout set. \n",
    "# Random state 1 and test size of 30%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train each of the 5 models using the training set. Tuning at least one hyperparameter and using the same tuning strategy across the 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Bagging\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Boosting\n",
    "\n",
    "# Tuning strategy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score # evaluation metrics 1\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score # evaluation metrics 2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time # For time evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "Following 4 step approach for all 5 methods\n",
    "1. Instantiate\n",
    "2. Set up hyperparamters to tune (Regularization or alpha tuning for first 2 and estimator or no. of trees for next 3 methods.)\n",
    "3. Use CV gridsearch with 5 folds for all methods to find best hyperparameters. \n",
    "4. Fit the training dataset and get best hyperparameters for that model\n",
    "5. Record time taken for each method for general comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning time in seconds: 100.15244936943054\n",
      "Best hyperparameters are {'clf__C': 1.0, 'clf__max_iter': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "#    ******** Method 1 : Logistic Regression ******** \n",
    "# Train and tune Logistic Regression\n",
    "# Regularised Regression, Set up pipeline and grid search\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "start = time.time() # record start time\n",
    "estimator = Pipeline( [('scale', StandardScaler()),\n",
    "                      ('clf', LogisticRegression(penalty = 'l1', \n",
    "                                                 solver = 'liblinear', \n",
    "                                                 random_state = 1000))] )\n",
    "grid_lr = {'clf__C': np.logspace(-10, 10, 21),\n",
    "       'clf__max_iter': np.linspace(1000,10000,10)}\n",
    "\n",
    "# Gridsearch CV\n",
    "clf_lr = GridSearchCV(estimator, grid_lr, cv = 5, scoring = 'f1', n_jobs = -1)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "end = time.time() # record end time\n",
    "print('Tuning time in seconds:',end-start)\n",
    "\n",
    "print(\"Best hyperparameters are\",clf_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning time in seconds: 150.25842452049255\n",
      "Refined hyperparameters are {'clf__C': 0.05722367659350217, 'clf__max_iter': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "# Refine search\n",
    "\n",
    "start1 = time.time() # record start time\n",
    "grid_lra = {'clf__C': np.logspace(-2, 1, 100), # We will narrow our search around the selected value, and use a much finer grid\n",
    "       'clf__max_iter': np.linspace(1000,10000,10)}\n",
    "\n",
    "# Gridsearch CV\n",
    "clf_lr2 = GridSearchCV(estimator, grid_lra, cv = 5, scoring = 'f1', n_jobs = -1)\n",
    "clf_lr2.fit(X_train, y_train)\n",
    "end = time.time() # record end time\n",
    "print('Tuning time in seconds:',end-start)\n",
    "\n",
    "print(\"Refined hyperparameters are\",clf_lr2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning time in seconds: 8.830337524414062\n",
      "Best Alpha is {'ccp_alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "#    ******** Method 2 : Decision Tree ******** \n",
    "# Train and tune Decision Tree\n",
    "\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "start = time.time() # record start time\n",
    "tree_clf =  DecisionTreeClassifier(random_state= 1,min_samples_leaf = 5) # dont need to scale data so have one input\n",
    "grid_dt = {'ccp_alpha': np.logspace(-2, 1, 100)} \n",
    "        \n",
    "# Gridsearch CV\n",
    "clf_dt = GridSearchCV(tree_clf, grid_dt, cv = 5, scoring = 'f1', n_jobs = -1)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "end = time.time() # record end time\n",
    "print('Tuning time in seconds:',end-start)\n",
    "\n",
    "print(\"Best Alpha is\",clf_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning time in seconds: 2269.7653517723083\n",
      "Best estimator(or number of trees) is {'max_depth': 7, 'n_estimators': 147}\n"
     ]
    }
   ],
   "source": [
    "#    ******** Method 3 : Bagged Tree ******** \n",
    "# Train and tune Bagged Tree\n",
    "# In bagging all features are used in each split\n",
    "\n",
    "start = time.time() # record start time\n",
    "bg_clf =  RandomForestClassifier(random_state= 1,min_samples_leaf = 5,max_features=None)\n",
    "grid_bt = {'n_estimators': np.linspace(100,1000,20, dtype = int),\n",
    "           'max_depth':range(2, 10)}\n",
    "\n",
    "# Gridsearch CV\n",
    "clf_bg = GridSearchCV(bg_clf, grid_bt, cv = 5, scoring = 'f1', n_jobs = -1)\n",
    "clf_bg.fit(X_train, y_train)\n",
    "end = time.time() # record end time\n",
    "print('Tuning time in seconds:',end-start)\n",
    "\n",
    "print(\"Best estimator(or number of trees) is\",clf_bg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning time in seconds: 93.29063391685486\n",
      "Best hyper parameters are {'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#    ******** Method 4 : Random Forest ******** \n",
    "# Train and tune Random Forest\n",
    "# In Random Forest add random subspace method all features are used in each split\n",
    "\n",
    "start = time.time() # record start time\n",
    "rf_clf = RandomForestClassifier(random_state= 1,min_samples_leaf = 5)\n",
    "grid_rt = {'n_estimators': np.linspace(100, 1000, 10, dtype = int),'max_features':['sqrt','log2'] }\n",
    "\n",
    "# Gridsearch CV\n",
    "clf_rt = GridSearchCV(rf_clf, grid_rt, cv = 5, n_jobs = -1, scoring = 'f1')\n",
    "clf_rt.fit(X_train, y_train)\n",
    "end = time.time() # record end time\n",
    "print('Tuning time in seconds:',end-start)\n",
    "\n",
    "print(\"Best hyper parameters are\",clf_rt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning time in seconds: 567.5045502185822\n",
      "Best hyper parameters are {'learning_rate': 0.05500000000000001, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#    ******** Method 5 : Boosted Trees ******** \n",
    "# Train and tune Boosted Trees\n",
    "\n",
    "start = time.time() # record start time\n",
    "bt_clf = GradientBoostingClassifier(random_state= 1,min_samples_leaf = 5)\n",
    "grid_bt = {'n_estimators': np.linspace(100, 1000, 10, dtype = int),'learning_rate': np.linspace(0.01,0.1,5)}\n",
    "\n",
    "# Gridsearch CV\n",
    "clf_bt = GridSearchCV(bt_clf, grid_bt, cv = 5, n_jobs = -1, scoring = 'f1')\n",
    "clf_bt.fit(X_train, y_train)\n",
    "end = time.time() # record end time\n",
    "print('Tuning time in seconds:',end-start)\n",
    "\n",
    "print(\"Best hyper parameters are\",clf_bt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform prediction on all 5 models using the holdout set, and report the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores on the test set are:\n",
      "Logistic Regression 0.898876404494382\n",
      "Decision Tree       0.8614232209737829\n",
      "Bagging             0.8856088560885609\n",
      "Random Forest       0.8913857677902622\n",
      "Boosting            0.8905109489051095\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import f1_score\n",
    "print(\"f1 scores on the test set are:\")\n",
    "print(\"Logistic Regression\",f1_score(y_test,clf_lr2.predict(X_test)))\n",
    "print(\"Decision Tree      \",f1_score(y_test,clf_dt.predict(X_test)))\n",
    "print(\"Bagging            \",f1_score(y_test,clf_bg.predict(X_test)))\n",
    "print(\"Random Forest      \",f1_score(y_test,clf_rt.predict(X_test)))\n",
    "print(\"Boosting           \",f1_score(y_test,clf_bt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**  \n",
    "**Logistic regression model** has the best performance, followed by **Random Forest** then **Boosting** and **Bagging**. Also, the time taken by Bagged tree and Boosting is the most but Logistic Regression is quite low comparatively yet providing a better f1 score.\n",
    "As expected, a single **Decision Tree** doesn't perform too well and gives lowest f1 score out of the five."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve is a good method to use when comparing multiple classification algorithms.\n",
    "Therefore, plotting the ROC curves of these 5 classifiers on the same graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC Curves\n",
    "\n",
    "# get y_score values\n",
    "y_prob1 = clf_lr2.predict_proba(X_test)[:,1]\n",
    "y_prob2 = clf_dt.predict_proba(X_test)[:,1]\n",
    "y_prob3 = clf_bg.predict_proba(X_test)[:,1]\n",
    "y_prob4 = clf_rt.predict_proba(X_test)[:,1]\n",
    "y_prob5 = clf_bt.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Setup ROC curve, y_true vs y_score\n",
    "vals1 = roc_curve(y_test, y_prob1, pos_label = 1) \n",
    "vals2 = roc_curve(y_test, y_prob2, pos_label = 1) \n",
    "vals3 = roc_curve(y_test, y_prob3, pos_label = 1)\n",
    "vals4 = roc_curve(y_test, y_prob4, pos_label = 1)\n",
    "vals5 = roc_curve(y_test, y_prob5, pos_label = 1)\n",
    "\n",
    "# plot graph for all methods\n",
    "plt.plot(vals1[0], vals1[1],color = 'b', label = 'Logistic Reg')\n",
    "plt.plot(vals2[0], vals2[1],color = 'g', label = 'Decision Tree')\n",
    "plt.plot(vals3[0], vals3[1],color = 'r', label = 'Bagging')\n",
    "plt.plot(vals4[0], vals4[1],color = 'c', label = 'RandomForest')\n",
    "plt.plot(vals5[0], vals5[1],color = 'm', label = 'Boosting')\n",
    "\n",
    "# plt.ylim(0.7,1.01) # Zoom in to see overlapping lines closely but no help and need AUC score\n",
    "\n",
    "# add labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**  \n",
    "The ROC curves show that Decision Tree has minimum area under curve (diagonal line worst scenario because it is as good as guessing). The other 4 methods perform much better and it is observed that the area is somewhere between 0.8 to 1 for all of them. AUC score will give a better idea about the maximum area under curve which is an indicator of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Scores are:\n",
      "Logistic Reg:  0.9668288087538611\n",
      "Decision Tree: 0.8878275677781098\n",
      "Bagging:       0.9656874733914871\n",
      "Random Forest: 0.9640479360852197\n",
      "Boosting:      0.9753707075373423\n"
     ]
    }
   ],
   "source": [
    "# AUC values\n",
    "print (\"AUC Scores are:\")\n",
    "print(\"Logistic Reg: \", roc_auc_score(y_test, y_prob1))\n",
    "print(\"Decision Tree:\", roc_auc_score(y_test, y_prob2))\n",
    "print(\"Bagging:      \", roc_auc_score(y_test, y_prob3))\n",
    "print(\"Random Forest:\", roc_auc_score(y_test, y_prob4))\n",
    "print(\"Boosting:     \", roc_auc_score(y_test, y_prob5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**  \n",
    "Model with largest Area Under Curve(AUC) is the best classification model.Boosting gives the best score out of all for the classification model.\n",
    "Although the scoring is very close, the order of best classification as per AUC score is **Boosting** followed by **Logistic Regression** then **Bagging** and then **Random Forest**.\n",
    "**Decision Tree** still has the lowest AUC (As seen in the graph as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Confusion Matrix \n",
      " [[122  25]\n",
      " [  5 746]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix # create confusion matrix\n",
    "\n",
    "yhat = clf_bt.predict(X_test)\n",
    "confmat = confusion_matrix(y_true= y_test, y_pred= yhat, labels = [1,0])       \n",
    "print(\"Final Confusion Matrix \\n\",confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**  \n",
    "Model selection based on f1 score and AUC score give different results because their criteria for a good model are different.\n",
    "\n",
    "f1 does not take the true negatives into account and is a measure of precision and recall at a particular threshold value. High \n",
    "f1 implies both precision and recall are high.\n",
    "\n",
    "f1-scoreï¼š 2/(1/P+1/R) where  \n",
    "Precision: TP/(TP+FP)  \n",
    "Recall: TP/(TP+FN)\n",
    "\n",
    "ROC/AUCï¼š TPR=TP/(TP+FN), FPR=FP/(FP+TN)\n",
    "\n",
    "For data like this as fraud detection, the positive examples(frauds) have relatively low rates of occurrence.\n",
    "Due to this unbalance in data f1 score gets affected but not ROC/AUC. With imbalanced data, the AUC still gives value around 0.8. However, it is high due to large FP, rather than the large TP (True positive).\n",
    "Therefore, f1 is a better evaluation metric in this case.\n",
    "Roc/Auc is good with balanced distribution of positives and negatives in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
